{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (1.50.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\genai-poc\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant that helps human by generating tutorials given a text.\n",
    "You will be provided with a text. If the text contains any kind of istructions on how to proceed with something, generate a tutorial in a bullet list.\n",
    "Otherwise, inform the user that the text does not contain any instructions.\n",
    "\n",
    "Text: \n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "Give me receipe for dinner based on ingredients such as Tomatoo, Pasta, onion, Panner, Salt, Brokoli.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AEsTZivdINLTeR9bsri52zhyfB2Wy', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sure, based on the ingredients you mentioned, here is a recipe for dinner!\\n\\nIngredients:\\n- Tomato\\n- Pasta\\n- Onion\\n- Paneer\\n- Salt\\n- Broccoli\\n\\nRecipe:\\n1. Cook the pasta according to the package instructions until al dente. Drain and set aside.\\n2. Heat some oil in a pan and sauté chopped onions until translucent.\\n3. Add diced paneer to the pan and cook until slightly browned.\\n4. Add chopped tomatoes and broccoli to the pan and cook until the vegetables are tender.\\n5. Season with salt and any other desired spices.\\n6. Finally, add the cooked pasta to the pan and mix everything together until well combined.\\n7. Serve hot and enjoy your delicious pasta with vegetables and paneer!\\n\\nI hope you enjoy preparing and eating this dinner recipe!', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728109361, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=167, prompt_tokens=106, total_tokens=273, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "client =  OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": instructions},\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, based on the ingredients you mentioned, here is a recipe for dinner!\\n\\nIngredients:\\n- Tomato\\n- Pasta\\n- Onion\\n- Paneer\\n- Salt\\n- Broccoli\\n\\nRecipe:\\n1. Cook the pasta according to the package instructions until al dente. Drain and set aside.\\n2. Heat some oil in a pan and sauté chopped onions until translucent.\\n3. Add diced paneer to the pan and cook until slightly browned.\\n4. Add chopped tomatoes and broccoli to the pan and cook until the vegetables are tender.\\n5. Season with salt and any other desired spices.\\n6. Finally, add the cooked pasta to the pan and mix everything together until well combined.\\n7. Serve hot and enjoy your delicious pasta with vegetables and paneer!\\n\\nI hope you enjoy preparing and eating this dinner recipe!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AHM1i9RKD3d9GereO0dDZsREtC4ew', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, but the text does not contain any instructions. If you need help with something specific, feel free to ask!\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728699610, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=89, total_tokens=115, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}))\n",
      "I'm sorry, but the text does not contain any instructions. If you need help with something specific, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Lets  Experiment , and pass text to the model which will not conatin any instructions, thus in that case model should say user not able to proceed since the text does not conain any instructions.\n",
    "'''\n",
    "client =  OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": 'the sun is shining and dogs are running on the beach.'},\n",
    "    ]\n",
    ")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Note:  if openai < 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Execute the following code, in openai == 0.28 \n",
    "'''\n",
    "\n",
    "# import os\n",
    "# import openai\n",
    "# openai.api_key = os.environment.get('OPENAI_API_KEY')\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\", # engine = \"deployment_name\".\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": system_message},\n",
    "#         {\"role\": \"user\", \"content\": instructions},\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask for justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    asking for justification might be useful also in case of\n",
    "    answers that are right but we simply don’t know the LLM’s \n",
    "    reasoning behind it. For example, let’s say we want our LLM to solve riddles. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are an AI assistant specialized in solving riddles.\n",
    "    Given a riddle, solve it the best you can.\n",
    "    Provide a clear justification of your answer and the reasoning behind it.\n",
    "    Riddle:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "riddle = \"\"\"\n",
    "I have a tail, and I have a head, but I have no body. I am NOT a snake. What am I?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AHMTzmhoMCReidcNL7qFqbqMJoP6O', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The answer to this riddle is a coin. \\n\\nExplanation:\\n1. A coin has a head side and a tail side, representing the head and tail mentioned in the riddle.\\n2. A coin is flat and round with no physical body, unlike a snake which has a body.\\n3. The riddle explicitly mentions that the answer is not a snake, eliminating that possibility and further pointing towards a coin as the solution.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728701363, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=86, prompt_tokens=83, total_tokens=169, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}))\n",
      "The answer to this riddle is a coin. \n",
      "\n",
      "Explanation:\n",
      "1. A coin has a head side and a tail side, representing the head and tail mentioned in the riddle.\n",
      "2. A coin is flat and round with no physical body, unlike a snake which has a body.\n",
      "3. The riddle explicitly mentions that the answer is not a snake, eliminating that possibility and further pointing towards a coin as the solution.\n"
     ]
    }
   ],
   "source": [
    "client =  OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": riddle},\n",
    "    ]\n",
    ")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate many outputs, then use the model to pick the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    rather than generating just one response,\n",
    "    we can prompt the model to generate multiple responses, \n",
    "    and then pick the one that is most suitable for the user’s query. \n",
    "    This splits the job into two subtasks for our LLM:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant specialized in solving riddles.\n",
    "Given a riddle, you have to generate three answers to the riddle.\n",
    "For each answer, be specific about the reasoning you made.\n",
    "Then, among the three answers, select the one that is most plausible given the riddle.\n",
    "Riddle:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "riddle = \"\"\"\n",
    "I have a tail, and I have a head, but I have no body. I am NOT a snake. What am I?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AHMa7kMWWhuce2DItV6sss0r4lpM1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Answer 1: Coin  \\nExplanation: Coins have a head (the side with a face or figure) and a tail (the opposite side with a design or number), but they do not have a body.\\n\\nAnswer 2: Arrow  \\nExplanation: Arrows have a pointed head and a tail fletching, but they do not have a body as they are just a straight line with these two parts.\\n\\nAnswer 3: Comet  \\nExplanation: Comets have a visible tail and a distinct head (nucleus), but they do not have a physical body like planets or asteroids.\\n\\nAmong the three answers, the most plausible answer to the riddle is \"Coin\" because coins commonly have a head and a tail, without having a physical body, matching the description in the riddle.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728701743, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=159, prompt_tokens=101, total_tokens=260, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}))\n",
      "Answer 1: Coin  \n",
      "Explanation: Coins have a head (the side with a face or figure) and a tail (the opposite side with a design or number), but they do not have a body.\n",
      "\n",
      "Answer 2: Arrow  \n",
      "Explanation: Arrows have a pointed head and a tail fletching, but they do not have a body as they are just a straight line with these two parts.\n",
      "\n",
      "Answer 3: Comet  \n",
      "Explanation: Comets have a visible tail and a distinct head (nucleus), but they do not have a physical body like planets or asteroids.\n",
      "\n",
      "Among the three answers, the most plausible answer to the riddle is \"Coin\" because coins commonly have a head and a tail, without having a physical body, matching the description in the riddle.\n"
     ]
    }
   ],
   "source": [
    "client =  OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": riddle},\n",
    "    ]\n",
    ")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formating our metaprompt (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    User can pass desired format to the metaprompt\n",
    "    this helps our LLM to better understand its intents as well as relate different sections and paragraphs to each other.\n",
    "    To achieve this, we can use delimiters within our prompt.\n",
    "    Delimiters mark distinct sections within a prompt, separating instructions, examples, and desired output.\n",
    "    Effective use of delimiters organizes instructions, inputs, and outputs, leading to coherent responses.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example: Write a python program to calculate the Factorial of a Number\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a Python expert who produces Python code as per the user's request.\n",
    "===>START EXAMPLE\n",
    "---User Query---\n",
    "Give me a function to print a string of text.\n",
    "---User Output---\n",
    "Below you can find the described function:\n",
    "```def my_print(text):\n",
    "     return print(text)\n",
    "```\n",
    "<===END EXAMPLE\n",
    "\"\"\"\n",
    "query = \"generate a Python function to to calculate the Factorial of a Number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AHNG1fNBJdiK1ABW2UYpkM8GeWGzD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Below you can find the described function:\\n\\n```python\\ndef calculate_factorial(number):\\n    factorial = 1\\n    if number < 0:\\n        return \"Factorial is not defined for negative numbers\"\\n    elif number == 0:\\n        return factorial\\n    else:\\n        for i in range(1, number + 1):\\n            factorial = factorial * i\\n        return factorial\\n```\\n\\nThis function takes a number as input and calculates the factorial of that number.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1728704341, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=97, prompt_tokens=91, total_tokens=188, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0), prompt_tokens_details={'cached_tokens': 0}))\n",
      "Below you can find the described function:\n",
      "\n",
      "```python\n",
      "def calculate_factorial(number):\n",
      "    factorial = 1\n",
      "    if number < 0:\n",
      "        return \"Factorial is not defined for negative numbers\"\n",
      "    elif number == 0:\n",
      "        return factorial\n",
      "    else:\n",
      "        for i in range(1, number + 1):\n",
      "            factorial = factorial * i\n",
      "        return factorial\n",
      "```\n",
      "\n",
      "This function takes a number as input and calculates the factorial of that number.\n"
     ]
    }
   ],
   "source": [
    "client =  OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    ")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI-POC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
